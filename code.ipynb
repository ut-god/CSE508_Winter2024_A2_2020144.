{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/utkarshpal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/utkarshpal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/utkarshpal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def pre_process_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    tokens = word_tokenize(sentence)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    filtered_tokens = [word for word in filtered_tokens if word.isalnum()]\n",
    "    stemming = PorterStemmer()\n",
    "    filtered_tokens = [stemming.stem(word) for word in filtered_tokens]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    filtered_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    filtered_tokens = list(filter(lambda token: token.strip() != '', filtered_tokens))\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-2a) Reading the data and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "dataset_df = pd.read_csv('A2_Data.csv', names=[\"ID\" , \"Image\" , \"Review Text\"])\n",
    "data = {}\n",
    "\n",
    "# Iterate over the rows of the DataFrame\n",
    "for index, row in dataset_df[1:].iterrows():\n",
    "    images = row[\"Image\"]\n",
    "    entry = {\"Text\":[] , \"Images\":[] , \"Original\": \"\"} \n",
    "    urls_list = images.strip(\"[]\").split(\", \")\n",
    "    urls_list = [url.strip(\"'\") for url in urls_list]\n",
    "    entry[\"Images\"] = urls_list\n",
    "    if pd.isnull(row[\"Review Text\"]):\n",
    "        review = [\"<no-review>\"] # This is a placeholder for the missing review\n",
    "        entry[\"Original\"] = \"\"\n",
    "    else:\n",
    "        entry[\"Original\"] = row[\"Review Text\"]\n",
    "        review = pre_process_sentence(row[\"Review Text\"])\n",
    "    entry[\"Text\"] = review\n",
    "    data[row[\"ID\"]] = entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {} # store features corresponding to a file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strip a link \n",
    "def strip_link(link):\n",
    "    return link.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to store the images\n",
    "if not os.path.exists('images'):\n",
    "    os.makedirs('images')\n",
    "\n",
    "ct =0\n",
    "for product_id in data.keys():\n",
    "    temp_del = []\n",
    "    for link in data[product_id][\"Images\"]:\n",
    "        response = requests.get(link)\n",
    "        if response.status_code == 200:\n",
    "            file_name = \"images/\"+ strip_link(link)\n",
    "            with open(file_name, 'wb') as image_file:\n",
    "                image_file.write(response.content)\n",
    "            try:\n",
    "                img = Image.open(file_name)\n",
    "                ct += 1\n",
    "            except:\n",
    "                temp_del.append(link)\n",
    "                os.remove(file_name)\n",
    "\n",
    "        else:\n",
    "            temp_del.append(link)\n",
    "    for link in temp_del:\n",
    "        data[product_id][\"Images\"].remove(link)\n",
    "\n",
    "# removes entries with no images\n",
    "for product_id in list(data.keys()):\n",
    "    if len(data[product_id][\"Images\"]) == 0:\n",
    "        del data[product_id]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-1 a) Pre-Process Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_image(image, angle):\n",
    "    return image.rotate(angle)\n",
    "\n",
    "def flip_image(image, flip):\n",
    "    return image.transpose(flip)\n",
    "\n",
    "def alter_brightness(image, factor):\n",
    "    np_image = np.array(image)\n",
    "    new_image = np_image + factor\n",
    "    new_image = np.clip(new_image, 0, 255)\n",
    "    new_image = Image.fromarray(np.uint8(new_image))\n",
    "    return new_image\n",
    "\n",
    "def alter_image(image):\n",
    "    angle = random.uniform(-45, 45)\n",
    "    flip = random.choice([Image.FLIP_LEFT_RIGHT, Image.FLIP_TOP_BOTTOM])\n",
    "    brightness_factor = random.uniform(-50, 50)\n",
    "    new_image = rotate_image(image, angle)\n",
    "    new_image = flip_image(new_image, flip)\n",
    "    new_image = alter_brightness(new_image, brightness_factor)\n",
    "    return new_image\n",
    "\n",
    "# Create a directory to store the altered images\n",
    "os.makedirs('altered_images', exist_ok=True)\n",
    "# Alter all the images and save them to the new directory\n",
    "for i in data.keys():\n",
    "    for link in data[i][\"Images\"]:        \n",
    "        try:   \n",
    "            image = Image.open('images/' + strip_link(link))\n",
    "            new_image = alter_image(image)\n",
    "            new_image.save('altered_images/' + strip_link(link))\n",
    "        except:\n",
    "            print(\"Error in image: \", i)\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-1 b/c) Use PreTrained Resnet50 to extract features and normalize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tensor(tensor):\n",
    "    norm = math.sqrt(sum([x**2 for x in tensor]))\n",
    "    return tensor / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/utkarshpal/Library/Python/3.9/lib/python/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/utkarshpal/Library/Python/3.9/lib/python/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = resnet50(pretrained=True)\n",
    "model.to(device)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),  # Resize the image to 256x256\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def extract_features(image_path):\n",
    "  img = Image.open(image_path)\n",
    "  img = transform(img)\n",
    "  img = img.unsqueeze(0)  \n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    features = model(img.to(device))\n",
    "  features = features.squeeze(0).flatten()\n",
    "  features = features/features.norm()\n",
    "  # features = features / normalize_tensor(features) # Normalize the features\n",
    "  return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle the features corresponding to every image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data.keys():\n",
    "    for link in data[i][\"Images\"]:\n",
    "        try:\n",
    "            features[strip_link(link)] = extract_features('altered_images/' + strip_link(link))\n",
    "        except:\n",
    "            print(\"Error in image: \", i)\n",
    "            continue\n",
    "with open('features.pkl', 'wb') as f:\n",
    "    pickle.dump(features, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the features\n",
    "with open('features.pkl', 'rb') as f:\n",
    "    features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-2 b) Caluclation of tf-idf matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set()\n",
    "for product_id in data.keys():\n",
    "    words = data[product_id][\"Text\"]\n",
    "    vocabulary.update(words)\n",
    "vocabulary = list(vocabulary)\n",
    "vocabulary.sort()\n",
    "\n",
    "df = {word: 0 for word in vocabulary}\n",
    "for product_id in data.keys():\n",
    "    words = data[product_id][\"Text\"]\n",
    "    for word in set(words):\n",
    "        df[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix: 994x4431\n"
     ]
    }
   ],
   "source": [
    "def calculate_tfidf(data):\n",
    "    tfidf = []\n",
    "    for product_id in data.keys():\n",
    "        words = data[product_id][\"Text\"]\n",
    "        tfidf_vector = []\n",
    "        for word in vocabulary:\n",
    "            tf = words.count(word) / (1+len(words)) # Add 1 to the denominator to prevent division by zero\n",
    "            idf = math.log(len(data) / df[word])\n",
    "            tfidf_value = tf * idf\n",
    "            tfidf_vector.append(tfidf_value)\n",
    "        data[product_id][\"tfidf\"] = tfidf_vector\n",
    "        tfidf.append(tfidf_vector)\n",
    "    \n",
    "    return tfidf\n",
    "\n",
    "tfidf = calculate_tfidf(data)\n",
    "print(f\"TF-IDF matrix: {len(tfidf)}x{len(tfidf[0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle tfidf\n",
    "with open('tfidf.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TF-IDF matrix from the file\n",
    "with open('tfidf.pkl', 'rb') as f:\n",
    "    tfidf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    dot_product = sum(x * y for x, y in zip(v1, v2))\n",
    "    norm1 = sum(x ** 2 for x in v1) ** 0.5\n",
    "    norm2 = sum(x ** 2 for x in v2) ** 0.5\n",
    "    cosine_similarity = dot_product / (norm1 * norm2)\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_tfidf_vector(text):\n",
    "    words = pre_process_sentence(text)\n",
    "    tfidf_vector = []\n",
    "    for word in vocabulary:\n",
    "        tf = words.count(word) / (1+len(words)) # Add 1 to the denominator to prevent division by zero\n",
    "        idf = math.log(len(data) / df[word])\n",
    "        tfidf_value = tf * idf\n",
    "        tfidf_vector.append(tfidf_value)\n",
    "    return tfidf_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-3) Text Based Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_text_query(query, data, tfidf, k=3):\n",
    "    similarities = [ (product , cosine_similarity(get_text_tfidf_vector(query), data[product][\"tfidf\"])) for product in data.keys()]\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    results = []\n",
    "    for i in range(k):\n",
    "        product_id = similarities[i][0]\n",
    "        product = data[product_id]\n",
    "        results.append({\"Index\": product_id, \"Text\": product[\"Original\"], \"Images\": product[\"Images\"], \"Similarity\": similarities[i][1]})\n",
    "    return results\n",
    "\n",
    "def text_based_retreival(query):\n",
    "    results = search_text_query(query[0], data, tfidf)\n",
    "    for result in results:\n",
    "        print(\"----------------------------------------------------------------\")\n",
    "        print(f\"Images URL: {result['Images']}\")\n",
    "        query_features = features[strip_link(query[1])]\n",
    "        similarity = sum([cosine_similarity(query_features, features[strip_link(image_link)] ) for image_link in result['Images']])/len(result['Images'])\n",
    "        print(f\"Product ID: {(result['Index'])}\")\n",
    "        print(f\"Review: {result['Text']}\")\n",
    "        print(f\"Cosine similarity of images: {similarity:.2f}\")\n",
    "        print(f\"Cosine similarity of text: {result['Similarity']:.2f}\")\n",
    "        print(f\"Composite similarity score:: {similarity*0.5 + result['Similarity']*0.5:.2f} \")    \n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "Images URL: ['https://images-na.ssl-images-amazon.com/images/I/71bztfqdg+L._SY88.jpg']\n",
      "Product ID: 654.0\n",
      "Review: I have been using Fender locking tuners for about five years on various strats and teles. Definitely helps with tuning stability and way faster to restring if there is a break.\n",
      "Cosine similarity of images: 1.00\n",
      "Cosine similarity of text: 1.00\n",
      "Composite similarity score:: 1.00 \n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Images URL: ['https://images-na.ssl-images-amazon.com/images/I/61DvLcapd8L._SY88.jpg']\n",
      "Product ID: 644.0\n",
      "Review: I went from fender chrome non-locking to fender gold locking. It made my guitar look beautiful and play beautiful. I think locking tuners are the way to go. If you are new to locking tuners look on YouTube for instructions.\n",
      "Cosine similarity of images: 0.76\n",
      "Cosine similarity of text: 0.27\n",
      "Composite similarity score:: 0.52 \n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Images URL: ['https://images-na.ssl-images-amazon.com/images/I/71mhnYAH5VL._SY88.jpg']\n",
      "Product ID: 3412.0\n",
      "Review: My Tele is perfect, thank you very much.\n",
      "Cosine similarity of images: 0.76\n",
      "Cosine similarity of text: 0.17\n",
      "Composite similarity score:: 0.47 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query =  [\"I have been using Fender locking tuners for about five years on various strats and teles. Definitely helps with tuning stability and way faster to restring ifthere is a break.\" \n",
    "          ,\"https://images-na.ssl-images-amazon.com/images/I/71bztfqdg+L._SY88.jpg\"]\n",
    "text_based_retreival(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-4) Image Based Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_image_query(query, k=5):\n",
    "    query_features = features[strip_link(query[1])]\n",
    "    similarities = []\n",
    "    for product_id in data.keys():\n",
    "        similarities_local = [ (cosine_similarity(query_features, features[strip_link(image_link)]) , image_link) for image_link in data[product_id][\"Images\"]]\n",
    "        similarities_local.sort(key=lambda x: x[0], reverse=True)\n",
    "        similarities.append(( similarities_local[0][0] , similarities_local[0][1] , product_id))\n",
    "    similarities.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    results = []\n",
    "    for i in range(k):\n",
    "        print(\"----------------------------------------------------------------\")\n",
    "        print(f\"Images URL: {similarities[i][1]}\")\n",
    "        print(f\"Product ID: {(similarities[i][2])}\")\n",
    "        print(f\"Review: {data[similarities[i][2]]['Original']}\")\n",
    "        print(f\"Cosine similarity of images: {similarities[i][0]:.2f}\")\n",
    "        text_similarity = (cosine_similarity(get_text_tfidf_vector(query[0]) , data[similarities[i][2]]['tfidf']) )\n",
    "        print(f\"Cosine similarity of text: { text_similarity}\")\n",
    "        # print(f\"Cosine similarity of text: { text_similarity:.2f}\")\n",
    "        print(f\"Composite similarity score:: {text_similarity*0.5 + similarities[i][0]*0.5:.2f} \")    \n",
    "        print()\n",
    "        print()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query =  [\"I have been using Fender locking tuners for about five years on various strats and teles. Definitely helps with tuning stability and way faster to restring ifthere is a break.\" \n",
    "          ,\"https://images-na.ssl-images-amazon.com/images/I/71bztfqdg+L._SY88.jpg\"]\n",
    "search_image_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-4 Combined Retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_combined_query(query , k =3):\n",
    "    query_features = features[strip_link(query[1])]\n",
    "    similarities = []\n",
    "    for product_id in data.keys():\n",
    "        similarities_local = [ (cosine_similarity(query_features, features[strip_link(image_link)]) , image_link) for image_link in data[product_id][\"Images\"]]\n",
    "        similarities_local.sort(key=lambda x: x[0], reverse=True)\n",
    "        similarities.append(( similarities_local[0][0] + cosine_similarity(get_text_tfidf_vector(query[0]), data[product_id][\"tfidf\"]) , similarities_local[0][1] , product_id))\n",
    "    \n",
    "    # Get the  top k most similar documents\n",
    "    similarities.sort(key=lambda x: x[0], reverse=True)\n",
    "    results = []\n",
    "    for i in range(k):\n",
    "        # product = data[product_id]\n",
    "        # results.append({\"Index\": product_id, \"Text\": ' '.join(product[\"Text\"]), \"Image\": similarities[i][1], \"Similarity\": similarities[i][0]})\n",
    "        print(\"----------------------------------------------------------------\")\n",
    "        print(f\"Images URL: {similarities[i][1]}\")\n",
    "        print(f\"Product ID: {(similarities[i][2])}\")\n",
    "        print(f\"Review: {data[similarities[i][2]]['Original']}\")\n",
    "        print(f\"Cosine similarity of images:{cosine_similarity(query_features,features[strip_link(similarities[i][1])]):.2f}\")\n",
    "\n",
    "        text_similarity = (cosine_similarity(get_text_tfidf_vector(query[0]),data[similarities[i][2]]['tfidf']) )\n",
    "        print(f\"Cosine similarity of text: { text_similarity}\")\n",
    "        print(f\"Composite similarity score:: {text_similarity*0.5 + similarities[i][0]*0.5:.2f} \")    \n",
    "        print()\n",
    "        print()\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "Images URL: https://images-na.ssl-images-amazon.com/images/I/71bztfqdg+L._SY88.jpg\n",
      "Product ID: 654.0\n",
      "Review: I have been using Fender locking tuners for about five years on various strats and teles. Definitely helps with tuning stability and way faster to restring if there is a break.\n",
      "Cosine similarity of images:1.00\n",
      "Cosine similarity of text: 1.0000000000000002\n",
      "Composite similarity score:: 1.50 \n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Images URL: https://images-na.ssl-images-amazon.com/images/I/61g0lol4mUL._SY88.jpg\n",
      "Product ID: 3772.0\n",
      "Review: Nice tuners.  Installed on a strat neck and they are working great. Nice and smooth and has stayed in tune very well. Nothing wrong with these.\n",
      "Cosine similarity of images:0.90\n",
      "Cosine similarity of text: 0.16327964800350983\n",
      "Composite similarity score:: 0.61 \n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Images URL: https://images-na.ssl-images-amazon.com/images/I/61clqkZnKxL._SY88.jpg\n",
      "Product ID: 655.0\n",
      "Review: Now all I have to do is install these on my Burswood Strat Copy. I know I'm gonna have to drill holes for the locating pins I may even have to drill the tuner holes also. Look forward to getting those crappy, loose, un-smooth, original hardware tuners off this thing.\n",
      "\n",
      "July 20, 2012:\n",
      "  Just installed these Fender locking tuners on my Strat. Didn't have to drill the tuner holes, they were the perfect size. Placed all the tuners in the headstock lined them up with a straight edge and just pressed each tuner very firmly into their hole (with the locking part screwed in tight so as not to damage them) and  very lightly and carefully tapped them with a small rubber mallet to mark my drill points. Then with a small magnifying glass and using a punch I made a center punch mark (just using pressure by hand with the punch since wood is soft) on each indent that the tuner made. Set my drill depth with a piece of tape on the drill and was done in less than 15 minutes.\n",
      "Cosine similarity of images:0.89\n",
      "Cosine similarity of text: 0.1619841880328972\n",
      "Composite similarity score:: 0.61 \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query =  [\"I have been using Fender locking tuners for about five years on various strats and teles. Definitely helps with tuning stability and way faster to restring ifthere is a break.\" ,\"https://images-na.ssl-images-amazon.com/images/I/71bztfqdg+L._SY88.jpg\"]\n",
    "search_combined_query(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
